# AWS Lambda Function for GCS to S3 File Transfer# google_storage_to_aws_s3_data_ingestion

This repository contains a Python script for an AWS Lambda function that automatically transfers files from Google Cloud Storage (GCS) to Amazon S3.
The code was written with a very specific need - ingestion of [DV360 data transfer files](https://developers.google.com/bid-manager/dtv2/overview) from Google storage to AWS S3 bucket.
There are 3 types of files generated by DV360 hence the architecture of the S3 bucket is be broken into 3 directories for 2 root buckets - one for development and one for production environment.
The intention of the bucket architecture and is to be used for multiple campaigns therefor we introduce standartised naming convention that includes the DSP name and the campaign name.
It is advisable also to add object tagging storage object tagging to be able to track precisely your ecpenses.

## Features

- Automatic file transfer from GCS to S3
- Scheduled daily execution
-  Special handling for 'impressions' files
- Error handling and logging
  
## Prerequisites

  - AWS account with access to Lambda, S3 and CloudWatch services
  - Google Cloud account with access to GCS
  - Python 3.8 or later

## Setup and Deployment

-  __Create S3 Buckets__: create s3 buckets for development and production with the relevant objects for impressions, activity and match files
- __Configure Google Cloud Storage__: ensure the GCS data transfer are set up and accessible
- __AWS Lambda Function__: write AWS lambda function using boto3 and google-cloud-storage libraries
- __Schedule Lambda Function__: schedule lambda function to run once a day using AWS Cloudwatch Events;
- __IAM Roles__: ensure the Lambda function has the necessary IAM roles for accessing the S3 buckets;
- __Environment Variables__: set the GOOGLE_APPLICATION_CREDENTIALS environment variable in the AWS Lambda function configuration to the path of your Google Cloud service account key file

Running the Tests

We use Python's built-in unittest module for testing. To run the tests, navigate to the project's root directory and run:

```
python -m unittest test
```
Please make sure to update the test script with your actual bucket names and other configuration details.

